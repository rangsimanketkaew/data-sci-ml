# ML techniques

## Dropout & Batch Normalization

- Dropout:
  - Used to avoid overfitting
  - Simple to implement
  - Widely adopted
- Batch Normalization:
  - Enables faster training / higher learning rates
  - Reduce the dependency over careful initialization
  - Widely adopted

- Apply dropout after the last batch normalization layer

## Optimizer

- AdamW (https://github.com/OverLordGoldDragon/keras-adamw)
